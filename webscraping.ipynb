{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Wille\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Wille\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In verband met dat er aan het begin tot aan 1 week staat dat het bijvoorbeeld 5 dagen geleden is word dat omgezet naar een absolute datum. Ook in het geval van 3 uur geleden word dit omgezet in een absolute datum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Functie om relatieve of absolute datums om te zetten naar een absolute datum\n",
    "def convert_date(date_str):\n",
    "    try:\n",
    "        # Controleer op relatieve datums zoals \"7 days ago\"\n",
    "        if \"day\" in date_str:\n",
    "            days_ago = int(date_str.split()[0])\n",
    "            return (datetime.now() - timedelta(days=days_ago)).date()\n",
    "        elif \"hour\" in date_str:\n",
    "            hours_ago = int(date_str.split()[0])\n",
    "            return (datetime.now() - timedelta(hours=hours_ago)).date()\n",
    "        elif \"minute\" in date_str:\n",
    "            minutes_ago = int(date_str.split()[0])\n",
    "            return (datetime.now() - timedelta(minutes=minutes_ago)).date()\n",
    "\n",
    "        # Als geen relatieve datum, probeer dan een absolute datum te parsen\n",
    "        # Formaat: \"Month Day, Year\" (bijvoorbeeld \"June 26, 2024\")\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), '%B %d, %Y').date()\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError while parsing date: {date_str}. Error: {ve}\")\n",
    "            return None  # Als parsing mislukt, retourneer None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting date: {date_str}. Exception: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beschrijving van de scraper\n",
    "\n",
    "De scraper begint met het initialiseren van de variabelen `news_data` (om de artikelen op te slaan) en `offset` (voor het bezoeken van de volgende pagina). Vervolgens wordt een header aangemaakt met een `User-Agent` om te voorkomen dat de scraper door de website als een bot wordt geïdentificeerd. De `offset` wordt gebruikt om na elke 10 artikelen door te gaan naar de volgende pagina van de resultaten.\n",
    "\n",
    "De HTML-structuur van de opgehaalde pagina wordt omgezet met BeautifulSoup om specifieke elementen te kunnen vinden. Daarna worden de links naar de artikelen opgehaald met behulp van `soup.find_all()`. Artikelen worden geïdentificeerd aan de hand van de bijbehorende HTML-classes.\n",
    "\n",
    "Vervolgens haalt de scraper de gegevens van elk artikel op, zoals de titel, de link, het fragment (excerpt), en de publicatiedatum. Als een artikel geen datum bevat, wordt dit artikel overgeslagen.\n",
    "\n",
    "Wat daarna gebeurt, is het omzetten van relatieve of absolute datums naar een uniform formaat (bijvoorbeeld een `datetime.date`). De functie biedt nu flexibiliteit: je kunt óf een specifieke einddatum (`until_date`) meegeven, óf een relatieve periode in dagen (`days_ago`). Als beide ontbreken, wordt standaard gekeken naar artikelen van maximaal 30 dagen geleden. Zodra een artikel wordt gevonden dat ouder is dan de opgegeven periode of datum, stopt de scraper. Dit is efficiënt, omdat de artikelen op de website meestal al op publicatiedatum worden gesorteerd.\n",
    "\n",
    "De opgehaalde attributen worden in het laatste gedeelte opgeslagen in een lijst genaamd `news_data`. Hierbij wordt de URL van elk artikel omgezet naar een volledige URL. Deze lijst wordt aan het einde van de functie geretourneerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "# Scraper functie voor nieuwsartikelen\n",
    "def scrape_news_financialpost(base_url, until_date=None, days_ago=None):\n",
    "    # Bepaal de until_date op basis van de parameters\n",
    "    try:\n",
    "        if until_date is None and days_ago is not None:\n",
    "            until_date = datetime.now().date() - timedelta(days=days_ago)\n",
    "        elif until_date is None:\n",
    "            # Standaard 30 dagen geleden als fallback\n",
    "            until_date = datetime.now().date() - timedelta(days=30)\n",
    "        else:\n",
    "            # Controleer of de opgegeven until_date een geldig datumformaat heeft\n",
    "            if isinstance(until_date, str):\n",
    "                try:\n",
    "                    until_date = datetime.strptime(until_date, '%Y-%m-%d').date()\n",
    "                except ValueError:\n",
    "                    raise ValueError(\"until_date is not in the correct format (expected 'YYYY-MM-DD').\")\n",
    "            elif not isinstance(until_date, datetime.date):\n",
    "                raise TypeError(\"until_date must be a string in 'YYYY-MM-DD' format or a datetime.date object.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting until_date: {e}\")\n",
    "        # Optioneel: fallback naar standaardwaarde\n",
    "        until_date = datetime.now().date() - timedelta(days=30)\n",
    "        print(f\"Fallback: until_date set to {until_date}\")\n",
    "    \n",
    "    news_data = []\n",
    "    offset = 0  # Start offset for pagination\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}&from={offset}\"\n",
    "        print(f\"Fetching URL: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all article containers\n",
    "        articles = soup.find_all('a', {'class': 'article-card__link'})\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"No more articles found. Stopping pagination.\")\n",
    "            break  # Stop if no articles are found on this page\n",
    "\n",
    "        for article in articles:\n",
    "            title_tag = article.find('h3', {'class': 'article-card__headline'})\n",
    "            excerpt_tag = article.find('p', {'class': 'article-card__excerpt'})\n",
    "            meta_bottom_tag = article.find_next('div', {'class': 'article-card__meta-bottom'})\n",
    "            date_tag = meta_bottom_tag.find('span', {'class': 'article-card__time-clamp'}) if meta_bottom_tag else None\n",
    "\n",
    "            if not date_tag:\n",
    "                print(\"HTML of the article without a date:\")\n",
    "                print(article.prettify())\n",
    "                continue  # Skip articles without a date\n",
    "\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "            link = article['href'] if article.has_attr('href') else None\n",
    "            excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else None\n",
    "            date_str = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "            # Parse the date if available\n",
    "            date = None\n",
    "            if date_str:\n",
    "                try:\n",
    "                    # Gebruik de dateutil parser\n",
    "                    date = parse(date_str).date()\n",
    "                except ValueError:\n",
    "                    date = convert_date(date_str)\n",
    "                    if date is None:\n",
    "                        print(f\"Error parsing date with convert_date: {date_str}. Skipping this article.\")\n",
    "                        continue\n",
    "\n",
    "\n",
    "            # Controleer consistent op datetime-objecten\n",
    "            if until_date and date and isinstance(date, datetime):\n",
    "                date = date.date()\n",
    "            if until_date and date < until_date:\n",
    "                print(f\"Article is older than {until_date}. Stopping.\")\n",
    "                return news_data\n",
    "\n",
    "            # Add valid articles to the list\n",
    "            if title and link:\n",
    "                news_data.append({\n",
    "                    'title': title,\n",
    "                    'link': f\"https://financialpost.com{link}\",\n",
    "                    'excerpt': excerpt,\n",
    "                    'date': date\n",
    "                })\n",
    "\n",
    "        # Pagination logic: increase offset by 10\n",
    "        offset += 10\n",
    "\n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "### Beschrijving van de functie `get_article_content`\n",
    "\n",
    "De functie `get_article_content` wordt gebruikt om de inhoud van een individueel artikel op te halen, op basis van de link die wordt meegegeven. Het proces begint met het toevoegen van een `User-Agent` header aan de aanvraag om toegang te krijgen tot de webpagina zonder dat de scraper als een bot wordt gezien.\n",
    "\n",
    "Vervolgens wordt een GET-verzoek verstuurd naar de opgegeven URL met de headers toegevoegd. Als het verzoek succesvol is (statuscode 200), wordt de HTML-inhoud van de pagina verwerkt met BeautifulSoup, wat zorgt voor een handige manier om de structuur van de pagina te navigeren en relevante data te extraheren.\n",
    "\n",
    "De scraper zoekt naar mogelijke secties van het artikel die verschillende HTML-tags kunnen bevatten, zoals `<div>`, `<section>`, en `<article>`. Dit gebeurt via de `find_all()` functie van BeautifulSoup, die meerdere secties teruggeeft die als mogelijke inhoud van het artikel dienen.\n",
    "\n",
    "Daarna itereren we door elke gevonden sectie en extraheren de alinea’s (`<p>` tags) binnen deze secties. Voor elke alinea wordt de tekst verzameld. Als er sterke tekst (zoals vetgedrukte woorden) wordt gevonden binnen een alinea, wordt deze eerst toegevoegd aan de inhoud.\n",
    "\n",
    "Het resultaat is de volledige tekst van het artikel, inclusief eventuele sterke tekst, die wordt samengevoegd en teruggegeven als één string. Indien de inhoud niet succesvol kan worden opgehaald (bijvoorbeeld bij een fout in het verzoek), wordt er een foutmelding weergegeven en retourneert de functie `None`.\n",
    "\n",
    "Deze aanpak maakt het mogelijk om gestructureerd en gedetailleerd de tekst van een artikel te verzamelen door meerdere secties en alinea’s van de pagina te doorzoeken en te extraheren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to scrape content from individual article\n",
    "def get_article_content(link):\n",
    "    # Voeg headers toe om toegang te krijgen tot de pagina\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Maak een GET-verzoek met de juiste headers\n",
    "    response = requests.get(link, headers=headers)\n",
    "\n",
    "    # Controleer of het verzoek succesvol was\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Zoek naar verschillende mogelijke secties van het artikel\n",
    "        possible_content = soup.find_all(['div', 'section', 'article'])  # Meer tags proberen\n",
    "\n",
    "        content = \"\"\n",
    "        \n",
    "        # Itereer over gevonden secties en probeer tekst te extraheren\n",
    "        for content_section in possible_content:\n",
    "            paragraphs = content_section.find_all('p')  # Zoek naar alinea's binnen de sectie\n",
    "            for para in paragraphs:\n",
    "                # Verkrijg de tekst en zoek naar sterke tekst als dat bestaat\n",
    "                strong_text = para.find('strong')\n",
    "                if strong_text:\n",
    "                    content += strong_text.get_text(strip=True) + \" \"\n",
    "                content += para.get_text(strip=True) + \" \"\n",
    "        \n",
    "        # Geef de content terug, indien gevonden\n",
    "        return content.strip()\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the article. Status code: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beschrijving van de functie `process_news`\n",
    "\n",
    "De functie `process_news` wordt gebruikt om de verzamelde nieuwsartikelen verder te verwerken en aan te vullen met extra informatie, zoals de inhoud van het artikel en de sentimentanalyse. Het begint met het initialiseren van een lege lijst genaamd `processed_news`, die zal worden gevuld met verrijkte gegevens van de artikelen.\n",
    "\n",
    "Eerst wordt de `SentimentIntensityAnalyzer` van de NLTK-bibliotheek geïnitialiseerd, evenals de `WordNetlemmatizer` voor het lemmatizeren van tokens. Vervolgens doorloopt de functie elk artikel in de meegegeven lijst `news_data`.\n",
    "\n",
    "Voor elk artikel worden de titel, de link en de publicatiedatum opgehaald. Daarna wordt de inhoud van het artikel opgehaald via de eerder gedefinieerde `get_article_content`-functie. Als de inhoud van het artikel niet beschikbaar is, wordt de titel gebruikt als alternatief voor de sentimentanalyse.\n",
    "\n",
    "De tekst (inhoud of titel) wordt vervolgens geanalyseerd met de `SentimentIntensityAnalyzer`, die een sentimentscore retourneert, die aangeeft of de tekst positief, negatief of neutraal is. Daarnaast wordt de tekst opgedeeld in tokens (woorden) via de functie `word_tokenize` en vervolgens lemmatized voor verdere tekstverwerking of analyse.\n",
    "\n",
    "De verrijkte gegevens, inclusief de titel, link, inhoud, tokens, sentimentanalyse en publicatiedatum, worden vervolgens toegevoegd aan de lijst `processed_news`.\n",
    "\n",
    "Na het doorlopen van alle artikelen, wordt de lijst `processed_news` geretourneerd, die nu alle benodigde informatie bevat voor verdere analyse of verwerking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Process and enrich news with content and sentiment\n",
    "def process_news(news_data):\n",
    "    processed_news = []\n",
    "\n",
    "    # Initialize SentimentIntensityAnalyzer and WordNetLemmatizer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for news in news_data:\n",
    "        title = news['title']\n",
    "        link = news['link']\n",
    "        date = news['date']  # Verkrijg de datum hier\n",
    "\n",
    "        # Scrape the article content\n",
    "        content = get_article_content(link)\n",
    "\n",
    "        # If no content is available, use the title for tokenization and sentiment analysis\n",
    "        text_to_analyze = content if content else title\n",
    "\n",
    "        # Sentiment Analysis\n",
    "        sentiment = sia.polarity_scores(text_to_analyze)\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text_to_analyze)\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Append enriched data, inclusief de datum\n",
    "        processed_news.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'content': content,\n",
    "            'tokens': lemmatized_tokens,\n",
    "            'sentiment': sentiment,\n",
    "            'date': date  # Voeg de datum toe aan de verwerkte data\n",
    "        })\n",
    "\n",
    "    return processed_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=0\n",
      "Error parsing date with dateutil: 17 minutes ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 19 hours ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 21 hours ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 23 hours ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 1 day ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 1 day ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 1 day ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 1 day ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 1 day ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 2 days ago. Trying convert_date function.\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=10\n",
      "Error parsing date with dateutil: 2 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 3 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 3 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 4 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 5 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 5 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 5 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 5 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 5 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 5 days ago. Trying convert_date function.\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=20\n",
      "Error parsing date with dateutil: 6 days ago. Trying convert_date function.\n",
      "Error parsing date with dateutil: 6 days ago. Trying convert_date function.\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=30\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=40\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=50\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=60\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=70\n",
      "Article is older than 2025-01-01. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Impax CEO Says Sudden Loss of $6 Billion Manda...</td>\n",
       "      <td>https://financialpost.com/pmn/business-pmn/imp...</td>\n",
       "      <td>The chief executive officer of Impax Asset Man...</td>\n",
       "      <td>[The, chief, executive, officer, of, Impax, As...</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.801, 'pos': 0.151, 'co...</td>\n",
       "      <td>2025-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stocks March Higher With Netflix Poised for Re...</td>\n",
       "      <td>https://financialpost.com/pmn/business-pmn/asi...</td>\n",
       "      <td>US equity futures signaled that the strong Wal...</td>\n",
       "      <td>[US, equity, future, signaled, that, the, stro...</td>\n",
       "      <td>{'neg': 0.026, 'neu': 0.837, 'pos': 0.137, 'co...</td>\n",
       "      <td>2025-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top Wall Street Banks Rake In ECM Revenue as I...</td>\n",
       "      <td>https://financialpost.com/pmn/business-pmn/top...</td>\n",
       "      <td>Wall Street’s biggest banks anticipate there’s...</td>\n",
       "      <td>[Wall, Street, ’, s, biggest, bank, anticipate...</td>\n",
       "      <td>{'neg': 0.014, 'neu': 0.858, 'pos': 0.128, 'co...</td>\n",
       "      <td>2025-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla slides, space stocks soar after Trump's ...</td>\n",
       "      <td>https://financialpost.com/news/tesla-slides-sp...</td>\n",
       "      <td>Could have wide-ranging impacts on markets Aut...</td>\n",
       "      <td>[Could, have, wide-ranging, impact, on, market...</td>\n",
       "      <td>{'neg': 0.023, 'neu': 0.827, 'pos': 0.151, 'co...</td>\n",
       "      <td>2025-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump tariff threats widen gap between U.S. an...</td>\n",
       "      <td>https://financialpost.com/commodities/energy/o...</td>\n",
       "      <td>U.S. energy stocks outperformed their northern...</td>\n",
       "      <td>[U.S., energy, stock, outperformed, their, nor...</td>\n",
       "      <td>{'neg': 0.022, 'neu': 0.844, 'pos': 0.134, 'co...</td>\n",
       "      <td>2025-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Corporate earnings growth faces uncertainty am...</td>\n",
       "      <td>https://financialpost.com/pmn/corporate-earnin...</td>\n",
       "      <td>Author of the article: You can save this artic...</td>\n",
       "      <td>[Author, of, the, article, :, You, can, save, ...</td>\n",
       "      <td>{'neg': 0.044, 'neu': 0.789, 'pos': 0.167, 'co...</td>\n",
       "      <td>2025-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Four considerations for your investments in th...</td>\n",
       "      <td>https://financialpost.com/investing/4-consider...</td>\n",
       "      <td>If you do not know how much you are paying in ...</td>\n",
       "      <td>[If, you, do, not, know, how, much, you, are, ...</td>\n",
       "      <td>{'neg': 0.023, 'neu': 0.849, 'pos': 0.128, 'co...</td>\n",
       "      <td>2025-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Here's what the CAPE ratio is signalling about...</td>\n",
       "      <td>https://financialpost.com/investing/what-cape-...</td>\n",
       "      <td>Noah Solomon: Investors are standing at a cros...</td>\n",
       "      <td>[Noah, Solomon, :, Investors, are, standing, a...</td>\n",
       "      <td>{'neg': 0.04, 'neu': 0.83, 'pos': 0.131, 'comp...</td>\n",
       "      <td>2025-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Wall Street Rattled by Rough Start to New Year...</td>\n",
       "      <td>https://financialpost.com/pmn/business-pmn/asi...</td>\n",
       "      <td>Major US benchmarks extended a selloff for a f...</td>\n",
       "      <td>[Major, US, benchmark, extended, a, selloff, f...</td>\n",
       "      <td>{'neg': 0.047, 'neu': 0.831, 'pos': 0.122, 'co...</td>\n",
       "      <td>2025-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>US Stocks Fall to Close Out Best Two-Year Stre...</td>\n",
       "      <td>https://financialpost.com/pmn/business-pmn/us-...</td>\n",
       "      <td>US stocks declined in the final session of the...</td>\n",
       "      <td>[US, stock, declined, in, the, final, session,...</td>\n",
       "      <td>{'neg': 0.029, 'neu': 0.824, 'pos': 0.147, 'co...</td>\n",
       "      <td>2025-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Impax CEO Says Sudden Loss of $6 Billion Manda...   \n",
       "1   Stocks March Higher With Netflix Poised for Re...   \n",
       "2   Top Wall Street Banks Rake In ECM Revenue as I...   \n",
       "3   Tesla slides, space stocks soar after Trump's ...   \n",
       "4   Trump tariff threats widen gap between U.S. an...   \n",
       "..                                                ...   \n",
       "69  Corporate earnings growth faces uncertainty am...   \n",
       "70  Four considerations for your investments in th...   \n",
       "71  Here's what the CAPE ratio is signalling about...   \n",
       "72  Wall Street Rattled by Rough Start to New Year...   \n",
       "73  US Stocks Fall to Close Out Best Two-Year Stre...   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://financialpost.com/pmn/business-pmn/imp...   \n",
       "1   https://financialpost.com/pmn/business-pmn/asi...   \n",
       "2   https://financialpost.com/pmn/business-pmn/top...   \n",
       "3   https://financialpost.com/news/tesla-slides-sp...   \n",
       "4   https://financialpost.com/commodities/energy/o...   \n",
       "..                                                ...   \n",
       "69  https://financialpost.com/pmn/corporate-earnin...   \n",
       "70  https://financialpost.com/investing/4-consider...   \n",
       "71  https://financialpost.com/investing/what-cape-...   \n",
       "72  https://financialpost.com/pmn/business-pmn/asi...   \n",
       "73  https://financialpost.com/pmn/business-pmn/us-...   \n",
       "\n",
       "                                              content  \\\n",
       "0   The chief executive officer of Impax Asset Man...   \n",
       "1   US equity futures signaled that the strong Wal...   \n",
       "2   Wall Street’s biggest banks anticipate there’s...   \n",
       "3   Could have wide-ranging impacts on markets Aut...   \n",
       "4   U.S. energy stocks outperformed their northern...   \n",
       "..                                                ...   \n",
       "69  Author of the article: You can save this artic...   \n",
       "70  If you do not know how much you are paying in ...   \n",
       "71  Noah Solomon: Investors are standing at a cros...   \n",
       "72  Major US benchmarks extended a selloff for a f...   \n",
       "73  US stocks declined in the final session of the...   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [The, chief, executive, officer, of, Impax, As...   \n",
       "1   [US, equity, future, signaled, that, the, stro...   \n",
       "2   [Wall, Street, ’, s, biggest, bank, anticipate...   \n",
       "3   [Could, have, wide-ranging, impact, on, market...   \n",
       "4   [U.S., energy, stock, outperformed, their, nor...   \n",
       "..                                                ...   \n",
       "69  [Author, of, the, article, :, You, can, save, ...   \n",
       "70  [If, you, do, not, know, how, much, you, are, ...   \n",
       "71  [Noah, Solomon, :, Investors, are, standing, a...   \n",
       "72  [Major, US, benchmark, extended, a, selloff, f...   \n",
       "73  [US, stock, declined, in, the, final, session,...   \n",
       "\n",
       "                                            sentiment        date  \n",
       "0   {'neg': 0.049, 'neu': 0.801, 'pos': 0.151, 'co...  2025-01-22  \n",
       "1   {'neg': 0.026, 'neu': 0.837, 'pos': 0.137, 'co...  2025-01-22  \n",
       "2   {'neg': 0.014, 'neu': 0.858, 'pos': 0.128, 'co...  2025-01-21  \n",
       "3   {'neg': 0.023, 'neu': 0.827, 'pos': 0.151, 'co...  2025-01-21  \n",
       "4   {'neg': 0.022, 'neu': 0.844, 'pos': 0.134, 'co...  2025-01-21  \n",
       "..                                                ...         ...  \n",
       "69  {'neg': 0.044, 'neu': 0.789, 'pos': 0.167, 'co...  2025-01-02  \n",
       "70  {'neg': 0.023, 'neu': 0.849, 'pos': 0.128, 'co...  2025-01-02  \n",
       "71  {'neg': 0.04, 'neu': 0.83, 'pos': 0.131, 'comp...  2025-01-02  \n",
       "72  {'neg': 0.047, 'neu': 0.831, 'pos': 0.122, 'co...  2025-01-01  \n",
       "73  {'neg': 0.029, 'neu': 0.824, 'pos': 0.147, 'co...  2025-01-01  \n",
       "\n",
       "[74 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL for Financial Post S&P 500 search\n",
    "base_url = \"https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc\"\n",
    "\n",
    "# Step 1: Scrape news articles to get links\n",
    "\n",
    "news_data = scrape_news_financialpost(base_url, until_date=\"2025-10-01\")\n",
    "\n",
    "# Step 2: Process news articles (scrape content, analyze sentiment)\n",
    "processed_news = process_news(news_data)\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "df_news = pd.DataFrame(processed_news)\n",
    "\n",
    "# Debugging: Show the first few rows of the DataFrame\n",
    "display(df_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue seems to be with the `isinstance` check in the `scrape_news_financialpost` function. The `until_date` is already a `datetime` object, but the check is not correctly handling it.\n",
    "\n",
    "Let's fix the `scrape_news_financialpost` function to correctly handle the `until_date` parameter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this updated version, the `scrape_news_financialpost` function correctly handles the `until_date` parameter by ensuring it is a `datetime.date` object. The `convert_date` function is used to handle relative dates like \"7 days ago\" and convert them to absolute dates. This should resolve the issues you were encountering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message indicates that there is an issue with how the `until_date` is being used within the `scrape_news_financialpost` function. Specifically, it seems like there is an `isinstance` check that is not correctly handling the `until_date`.\n",
    "\n",
    "Let's update the `scrape_news_financialpost` function to ensure it correctly handles the `until_date` parameter. Here's an example of how you might modify the function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example, the `scrape_news_financialpost` function checks if `until_date` is a `datetime` object and raises a `ValueError` if it is not. It then uses the `until_date` to filter out articles that are published after the specified date.\n",
    "\n",
    "Make sure to adjust the scraping logic according to your actual implementation. If you provide the actual implementation of `scrape_news_financialpost`, I can give more specific guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>article</th>\n",
       "      <th>articles</th>\n",
       "      <th>canada</th>\n",
       "      <th>comments</th>\n",
       "      <th>continue</th>\n",
       "      <th>create</th>\n",
       "      <th>experience</th>\n",
       "      <th>follow</th>\n",
       "      <th>freehere</th>\n",
       "      <th>...</th>\n",
       "      <th>latest</th>\n",
       "      <th>news</th>\n",
       "      <th>orsign</th>\n",
       "      <th>postmedia</th>\n",
       "      <th>read</th>\n",
       "      <th>reading</th>\n",
       "      <th>registering</th>\n",
       "      <th>save</th>\n",
       "      <th>sign</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.319653</td>\n",
       "      <td>0.228323</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>0.228323</td>\n",
       "      <td>0.182659</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>0.213102</td>\n",
       "      <td>0.167437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.273988</td>\n",
       "      <td>0.167437</td>\n",
       "      <td>0.213102</td>\n",
       "      <td>0.197880</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>0.167437</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>Impax CEO Says Sudden Loss of $6 Billion Manda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412543</td>\n",
       "      <td>0.333207</td>\n",
       "      <td>0.238005</td>\n",
       "      <td>0.190404</td>\n",
       "      <td>0.238005</td>\n",
       "      <td>0.190404</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.174537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253872</td>\n",
       "      <td>0.253872</td>\n",
       "      <td>0.174537</td>\n",
       "      <td>0.222138</td>\n",
       "      <td>0.206271</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.174537</td>\n",
       "      <td>0.253872</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>Stocks March Higher With Netflix Poised for Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.437733</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.252538</td>\n",
       "      <td>0.202031</td>\n",
       "      <td>0.252538</td>\n",
       "      <td>0.202031</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>0.185195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>0.185195</td>\n",
       "      <td>0.134687</td>\n",
       "      <td>0.218866</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>0.185195</td>\n",
       "      <td>0.269374</td>\n",
       "      <td>0.168359</td>\n",
       "      <td>Top Wall Street Banks Rake In ECM Revenue as I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407846</td>\n",
       "      <td>0.329414</td>\n",
       "      <td>0.235296</td>\n",
       "      <td>0.282355</td>\n",
       "      <td>0.235296</td>\n",
       "      <td>0.188237</td>\n",
       "      <td>0.156864</td>\n",
       "      <td>0.156864</td>\n",
       "      <td>0.156864</td>\n",
       "      <td>0.172550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156864</td>\n",
       "      <td>0.282355</td>\n",
       "      <td>0.172550</td>\n",
       "      <td>0.219610</td>\n",
       "      <td>0.203923</td>\n",
       "      <td>0.156864</td>\n",
       "      <td>0.172550</td>\n",
       "      <td>0.250982</td>\n",
       "      <td>0.156864</td>\n",
       "      <td>Tesla slides, space stocks soar after Trump's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.429532</td>\n",
       "      <td>0.346930</td>\n",
       "      <td>0.247807</td>\n",
       "      <td>0.297368</td>\n",
       "      <td>0.247807</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.181725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.181725</td>\n",
       "      <td>0.132164</td>\n",
       "      <td>0.214766</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>0.181725</td>\n",
       "      <td>0.264327</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>Trump tariff threats widen gap between U.S. an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.443296</td>\n",
       "      <td>0.358047</td>\n",
       "      <td>0.255748</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.255748</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.187548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.187548</td>\n",
       "      <td>0.136399</td>\n",
       "      <td>0.221648</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>0.187548</td>\n",
       "      <td>0.272798</td>\n",
       "      <td>0.170499</td>\n",
       "      <td>Corporate earnings growth faces uncertainty am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.456066</td>\n",
       "      <td>0.243235</td>\n",
       "      <td>0.228033</td>\n",
       "      <td>0.334448</td>\n",
       "      <td>0.258437</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.167224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.273640</td>\n",
       "      <td>0.167224</td>\n",
       "      <td>0.212831</td>\n",
       "      <td>0.197629</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.167224</td>\n",
       "      <td>0.243235</td>\n",
       "      <td>0.182426</td>\n",
       "      <td>Four considerations for your investments in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.415960</td>\n",
       "      <td>0.255975</td>\n",
       "      <td>0.239977</td>\n",
       "      <td>0.255975</td>\n",
       "      <td>0.239977</td>\n",
       "      <td>0.191982</td>\n",
       "      <td>0.159985</td>\n",
       "      <td>0.159985</td>\n",
       "      <td>0.159985</td>\n",
       "      <td>0.175983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159985</td>\n",
       "      <td>0.287972</td>\n",
       "      <td>0.175983</td>\n",
       "      <td>0.223978</td>\n",
       "      <td>0.207980</td>\n",
       "      <td>0.159985</td>\n",
       "      <td>0.175983</td>\n",
       "      <td>0.255975</td>\n",
       "      <td>0.223978</td>\n",
       "      <td>Here's what the CAPE ratio is signalling about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.412543</td>\n",
       "      <td>0.333207</td>\n",
       "      <td>0.238005</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.238005</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.174537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253872</td>\n",
       "      <td>0.253872</td>\n",
       "      <td>0.174537</td>\n",
       "      <td>0.222138</td>\n",
       "      <td>0.206271</td>\n",
       "      <td>0.190404</td>\n",
       "      <td>0.174537</td>\n",
       "      <td>0.253872</td>\n",
       "      <td>0.190404</td>\n",
       "      <td>Wall Street Rattled by Rough Start to New Year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.410686</td>\n",
       "      <td>0.331708</td>\n",
       "      <td>0.236934</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>0.236934</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252730</td>\n",
       "      <td>0.284321</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>0.221138</td>\n",
       "      <td>0.236934</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>0.252730</td>\n",
       "      <td>0.157956</td>\n",
       "      <td>US Stocks Fall to Close Out Best Two-Year Stre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     account   article  articles    canada  comments  continue    create  \\\n",
       "0   0.456647  0.319653  0.228323  0.152216  0.228323  0.182659  0.152216   \n",
       "1   0.412543  0.333207  0.238005  0.190404  0.238005  0.190404  0.158670   \n",
       "2   0.437733  0.353553  0.252538  0.202031  0.252538  0.202031  0.168359   \n",
       "3   0.407846  0.329414  0.235296  0.282355  0.235296  0.188237  0.156864   \n",
       "4   0.429532  0.346930  0.247807  0.297368  0.247807  0.165205  0.165205   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "69  0.443296  0.358047  0.255748  0.170499  0.255748  0.170499  0.170499   \n",
       "70  0.456066  0.243235  0.228033  0.334448  0.258437  0.152022  0.152022   \n",
       "71  0.415960  0.255975  0.239977  0.255975  0.239977  0.191982  0.159985   \n",
       "72  0.412543  0.333207  0.238005  0.158670  0.238005  0.158670  0.158670   \n",
       "73  0.410686  0.331708  0.236934  0.157956  0.236934  0.157956  0.157956   \n",
       "\n",
       "    experience    follow  freehere  ...    latest      news    orsign  \\\n",
       "0     0.152216  0.213102  0.167437  ...  0.243545  0.273988  0.167437   \n",
       "1     0.158670  0.158670  0.174537  ...  0.253872  0.253872  0.174537   \n",
       "2     0.168359  0.168359  0.185195  ...  0.168359  0.168359  0.185195   \n",
       "3     0.156864  0.156864  0.172550  ...  0.156864  0.282355  0.172550   \n",
       "4     0.165205  0.165205  0.181725  ...  0.165205  0.165205  0.181725   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "69    0.170499  0.170499  0.187548  ...  0.170499  0.170499  0.187548   \n",
       "70    0.152022  0.152022  0.167224  ...  0.152022  0.273640  0.167224   \n",
       "71    0.159985  0.159985  0.175983  ...  0.159985  0.287972  0.175983   \n",
       "72    0.158670  0.158670  0.174537  ...  0.253872  0.253872  0.174537   \n",
       "73    0.157956  0.157956  0.173752  ...  0.252730  0.284321  0.173752   \n",
       "\n",
       "    postmedia      read   reading  registering      save      sign  \\\n",
       "0    0.213102  0.197880  0.152216     0.167437  0.243545  0.152216   \n",
       "1    0.222138  0.206271  0.158670     0.174537  0.253872  0.158670   \n",
       "2    0.134687  0.218866  0.168359     0.185195  0.269374  0.168359   \n",
       "3    0.219610  0.203923  0.156864     0.172550  0.250982  0.156864   \n",
       "4    0.132164  0.214766  0.165205     0.181725  0.264327  0.165205   \n",
       "..        ...       ...       ...          ...       ...       ...   \n",
       "69   0.136399  0.221648  0.170499     0.187548  0.272798  0.170499   \n",
       "70   0.212831  0.197629  0.152022     0.167224  0.243235  0.182426   \n",
       "71   0.223978  0.207980  0.159985     0.175983  0.255975  0.223978   \n",
       "72   0.222138  0.206271  0.190404     0.174537  0.253872  0.190404   \n",
       "73   0.221138  0.236934  0.157956     0.173752  0.252730  0.157956   \n",
       "\n",
       "                                                title  \n",
       "0   Impax CEO Says Sudden Loss of $6 Billion Manda...  \n",
       "1   Stocks March Higher With Netflix Poised for Re...  \n",
       "2   Top Wall Street Banks Rake In ECM Revenue as I...  \n",
       "3   Tesla slides, space stocks soar after Trump's ...  \n",
       "4   Trump tariff threats widen gap between U.S. an...  \n",
       "..                                                ...  \n",
       "69  Corporate earnings growth faces uncertainty am...  \n",
       "70  Four considerations for your investments in th...  \n",
       "71  Here's what the CAPE ratio is signalling about...  \n",
       "72  Wall Street Rattled by Rough Start to New Year...  \n",
       "73  US Stocks Fall to Close Out Best Two-Year Stre...  \n",
       "\n",
       "[74 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combineer de content en title als een enkele tekst\n",
    "corpus = [f\"{news['title']} {news['content']}\" for news in processed_news]\n",
    "\n",
    "# Zet de tekst om naar een TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=20, stop_words='english')  # Verhoog max_features om meer woorden te krijgen\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Verkrijg de feature-namen (de belangrijkste woorden)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Zet de TF-IDF waarden om naar een array\n",
    "tfidf_values = X_tfidf.toarray()\n",
    "\n",
    "# Maak een DataFrame met de TF-IDF waarden, waarbij de kolommen de woorden zijn\n",
    "df_tfidf = pd.DataFrame(tfidf_values, columns=feature_names)\n",
    "\n",
    "# Voeg de titels toe aan de DataFrame als een extra kolom\n",
    "df_tfidf['title'] = [news['title'] for news in processed_news]\n",
    "\n",
    "# Toon de DataFrame\n",
    "display(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=0\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=10\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=20\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=30\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=40\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=50\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=60\n",
      "Fetching URL: https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc&from=70\n",
      "Article is older than 2025-01-01. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self, base_url, until_date=None, days_ago=None):\n",
    "        \"\"\"\n",
    "        Initializes the web scraping object with the given base URL and optional date parameters.\n",
    "\n",
    "        Args:\n",
    "            base_url (str): The base URL for the web scraping.\n",
    "            until_date (str, optional): The end date for the web scraping in 'YYYY-MM-DD' format. Defaults to None.\n",
    "            days_ago (int, optional): The number of days ago from today to set the end date. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            base_url (str): The base URL for the web scraping.\n",
    "            until_date (str): The calculated end date for the web scraping.\n",
    "            news_data (list): A list to store the scraped news data.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.until_date = self.set_until_date(until_date, days_ago)\n",
    "        self.news_data = []\n",
    "\n",
    "    def set_until_date(self, until_date, days_ago):\n",
    "        \"\"\"\n",
    "        Set the 'until_date' based on the provided parameters.\n",
    "\n",
    "        Parameters:\n",
    "        until_date (str or datetime.date or None): The target date until which to set. \n",
    "            If a string is provided, it should be in 'YYYY-MM-DD' format.\n",
    "            If None, the date will be calculated based on 'days_ago'.\n",
    "        days_ago (int or None): The number of days ago from today to set the 'until_date'.\n",
    "            If None, defaults to 30 days ago.\n",
    "\n",
    "        Returns:\n",
    "        datetime.date: The calculated 'until_date'.\n",
    "\n",
    "        Raises:\n",
    "        ValueError: If 'until_date' is a string but not in the correct 'YYYY-MM-DD' format.\n",
    "        TypeError: If 'until_date' is not a string or datetime.date object.\n",
    "\n",
    "        Notes:\n",
    "        - If both 'until_date' and 'days_ago' are None, the function defaults to 30 days ago.\n",
    "        - If an error occurs, the function prints an error message and defaults to 30 days ago.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if until_date is None and days_ago is not None:\n",
    "                return datetime.now().date() - timedelta(days=days_ago)\n",
    "            elif until_date is None:\n",
    "                return datetime.now().date() - timedelta(days=30)\n",
    "            else:\n",
    "                if isinstance(until_date, str):\n",
    "                    try:\n",
    "                        return datetime.strptime(until_date, '%Y-%m-%d').date()\n",
    "                    except ValueError:\n",
    "                        raise ValueError(\"until_date is not in the correct format (expected 'YYYY-MM-DD').\")\n",
    "                elif not isinstance(until_date, datetime.date):\n",
    "                    raise TypeError(\"until_date must be a string in 'YYYY-MM-DD' format or a datetime.date object.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting until_date: {e}\")\n",
    "            return datetime.now().date() - timedelta(days=30)\n",
    "\n",
    "    def convert_date(self, date_str):\n",
    "        \"\"\"\n",
    "        Converts a relative or absolute date string into a date object.\n",
    "        The function handles relative date strings such as \"2 days ago\", \"3 hours ago\",\n",
    "        \"15 minutes ago\", and \"1 week ago\". It also handles absolute date strings in the\n",
    "        format \"Month Day, Year\" (e.g., \"January 1, 2020\").\n",
    "        Args:\n",
    "            date_str (str): The date string to convert.\n",
    "        Returns:\n",
    "            date: A date object representing the converted date, or None if the conversion fails.\n",
    "        Raises:\n",
    "            ValueError: If the date string is in an unrecognized format.\n",
    "            Exception: For any other exceptions that occur during conversion.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if \"day\" in date_str:\n",
    "                days_ago = int(date_str.split()[0])\n",
    "                return (datetime.now() - timedelta(days=days_ago)).date()\n",
    "            elif \"hour\" in date_str:\n",
    "                hours_ago = int(date_str.split()[0])\n",
    "                return (datetime.now() - timedelta(hours=hours_ago)).date()\n",
    "            elif \"minute\" in date_str:\n",
    "                minutes_ago = int(date_str.split()[0])\n",
    "                return (datetime.now() - timedelta(minutes=minutes_ago)).date()\n",
    "            elif \"week\" in date_str:\n",
    "                weeks_ago = int(date_str.split()[0])\n",
    "                return (datetime.now() - timedelta(weeks=weeks_ago)).date()\n",
    "\n",
    "            try:\n",
    "                return datetime.strptime(date_str.strip(), '%B %d, %Y').date()\n",
    "            except ValueError as ve:\n",
    "                print(f\"ValueError while parsing date: {date_str}. Error: {ve}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting date: {date_str}. Exception: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_news(self):\n",
    "        \"\"\"\n",
    "        Scrapes news articles from the specified base URL with pagination.\n",
    "        This method fetches news articles by sending HTTP GET requests to the base URL with an offset parameter.\n",
    "        It parses the HTML content to extract article details such as title, link, excerpt, and date.\n",
    "        The method continues to paginate until no more articles are found or an article older than the specified `until_date` is encountered.\n",
    "        Returns:\n",
    "            list: A list of dictionaries, each containing the following keys:\n",
    "                - 'title' (str): The title of the article.\n",
    "                - 'link' (str): The full URL to the article.\n",
    "                - 'excerpt' (str): A brief excerpt of the article.\n",
    "                - 'date' (datetime.date): The publication date of the article.\n",
    "        Raises:\n",
    "            ValueError: If the date string cannot be parsed and `convert_date` method also fails to convert it.\n",
    "        Notes:\n",
    "            - The method uses a User-Agent header to mimic a browser request.\n",
    "            - The method prints debug information such as the URL being fetched and any errors encountered during parsing.\n",
    "            - The method stops pagination if no more articles are found or if an article is older than the specified `until_date`.\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{self.base_url}&from={offset}\"\n",
    "            print(f\"Fetching URL: {url}\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            articles = soup.find_all('a', {'class': 'article-card__link'})\n",
    "            \n",
    "            if not articles:\n",
    "                print(\"No more articles found. Stopping pagination.\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                title_tag = article.find('h3', {'class': 'article-card__headline'})\n",
    "                excerpt_tag = article.find('p', {'class': 'article-card__excerpt'})\n",
    "                meta_bottom_tag = article.find_next('div', {'class': 'article-card__meta-bottom'})\n",
    "                date_tag = meta_bottom_tag.find('span', {'class': 'article-card__time-clamp'}) if meta_bottom_tag else None\n",
    "\n",
    "                if not date_tag:\n",
    "                    print(\"HTML of the article without a date:\")\n",
    "                    print(article.prettify())\n",
    "                    continue\n",
    "\n",
    "                title = title_tag.get_text(strip=True) if title_tag else None\n",
    "                link = article['href'] if article.has_attr('href') else None\n",
    "                excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else None\n",
    "                date_str = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "                date = None\n",
    "                if date_str:\n",
    "                    try:\n",
    "                        date = parse(date_str).date()\n",
    "                    except ValueError:\n",
    "                        date = self.convert_date(date_str)\n",
    "                        if date is None:\n",
    "                            print(f\"Error parsing date with convert_date: {date_str}. Skipping this article.\")\n",
    "                            continue\n",
    "\n",
    "                if self.until_date and date and isinstance(date, datetime):\n",
    "                    date = date.date()\n",
    "                if self.until_date and date < self.until_date:\n",
    "                    print(f\"Article is older than {self.until_date}. Stopping.\")\n",
    "                    return self.news_data\n",
    "\n",
    "                if title and link:\n",
    "                    self.news_data.append({\n",
    "                        'title': title,\n",
    "                        'link': f\"https://financialpost.com{link}\",\n",
    "                        'excerpt': excerpt,\n",
    "                        'date': date\n",
    "                    })\n",
    "\n",
    "            offset += 10\n",
    "\n",
    "        return self.news_data\n",
    "\n",
    "    def get_article_content(self, link):\n",
    "        \"\"\"\n",
    "        Retrieves the content of an article from the given URL.\n",
    "        This method sends a GET request to the specified link with a custom User-Agent header.\n",
    "        If the request is successful (status code 200), it parses the HTML content using BeautifulSoup\n",
    "        and extracts text from all <p> tags within <div>, <section>, or <article> tags. If a <p> tag\n",
    "        contains a <strong> tag, the text within the <strong> tag is given priority.\n",
    "        Args:\n",
    "            link (str): The URL of the article to retrieve.\n",
    "        Returns:\n",
    "            str: The extracted article content as a single string, or None if the request failed.\n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: If there is an issue with the network request.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(link, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            possible_content = soup.find_all(['div', 'section', 'article'])\n",
    "\n",
    "            content = \"\"\n",
    "            for content_section in possible_content:\n",
    "                paragraphs = content_section.find_all('p')\n",
    "                for para in paragraphs:\n",
    "                    strong_text = para.find('strong')\n",
    "                    if strong_text:\n",
    "                        content += strong_text.get_text(strip=True) + \" \"\n",
    "                    content += para.get_text(strip=True) + \" \"\n",
    "            \n",
    "            return content.strip()\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the article. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def process_news(news_data):\n",
    "        processed_news = []\n",
    "\n",
    "        # Initialize SentimentIntensityAnalyzer and WordNetLemmatizer\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        for news in news_data:\n",
    "            title = news['title']\n",
    "            link = news['link']\n",
    "            date = news['date']  # Verkrijg de datum hier\n",
    "\n",
    "            # Scrape the article content\n",
    "            content = get_article_content(link)\n",
    "\n",
    "            # If no content is available, use the title for tokenization and sentiment analysis\n",
    "            text_to_analyze = content if content else title\n",
    "\n",
    "            # Sentiment Analysis\n",
    "            sentiment = sia.polarity_scores(text_to_analyze)\n",
    "\n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text_to_analyze)\n",
    "\n",
    "            # Lemmatization\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "            # Append enriched data, inclusief de datum\n",
    "            processed_news.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'content': content,\n",
    "                'tokens': lemmatized_tokens,\n",
    "                'sentiment': sentiment,\n",
    "                'date': date  # Voeg de datum toe aan de verwerkte data\n",
    "            })\n",
    "\n",
    "        return processed_news\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the entire workflow in the correct sequence.\n",
    "        \"\"\"\n",
    "        news_data = self.scrape_news()\n",
    "        processed_news = process_news(news_data)\n",
    "        return processed_news\n",
    "        \n",
    "# Voorbeeld van hoe de class te gebruiken\n",
    "scraper = NewsScraper(base_url='https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc', until_date='2025-01-01')\n",
    "news_data = scraper.scrape_news()\n",
    "\n",
    "NewsScraper.run(base_url='https://financialpost.com/search/?search_text=S%26P+500&date_range=-3650d&sort=desc', until_date='2025-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
